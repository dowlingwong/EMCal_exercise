{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ac0efad",
   "metadata": {},
   "source": [
    "\n",
    "# K-Means Clustering — Steps, Code, and Question Design\n",
    "\n",
    "This notebook is a compact, practical guide to K-Means. It includes:\n",
    "- Clear step-by-step instructions for implementing K-Means\n",
    "- A from-scratch implementation (NumPy)\n",
    "- A scikit-learn implementation\n",
    "- Evaluation and model selection (inertia, elbow, silhouette)\n",
    "- Preprocessing considerations (scaling, dimensionality)\n",
    "- Ready-to-use exercises and question prompts you can assign or self-test with\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98966c00",
   "metadata": {},
   "source": [
    "\n",
    "## Step-by-step instructions (Algorithm)\n",
    "1. **Choose K** (number of clusters).   Use domain knowledge or try several K values (elbow/silhouette).\n",
    "2. **Initialize centroids** (randomly or with **k-means++**).\n",
    "3. **Assign points** to the nearest centroid (Euclidean distance by default).\n",
    "4. **Update centroids** as the mean of points assigned to each cluster.\n",
    "5. **Check convergence**: stop when assignments don't change or centroid shift < tolerance, or when max iterations reached.\n",
    "6. **Evaluate** using inertia (within-cluster SSE) and/or **silhouette score**; refine K and preprocessing as needed.\n",
    "7. **Post-process**: label, visualize, interpret clusters, and validate with downstream tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611ad44e",
   "metadata": {},
   "source": [
    "\n",
    "## K-Means from scratch (NumPy)\n",
    "Implementation details:\n",
    "- Random initialization with an option for k-means++\n",
    "- Euclidean distance\n",
    "- Vectorized operations where possible\n",
    "- Returns labels, centroids, inertia, and number of iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0d1194",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def _euclidean_squared(a, b):\n",
    "    # a: (n_samples, n_features), b: (n_clusters, n_features)\n",
    "    # returns (n_samples, n_clusters)\n",
    "    a2 = np.sum(a*a, axis=1, keepdims=True)\n",
    "    b2 = np.sum(b*b, axis=1)\n",
    "    ab = a @ b.T\n",
    "    d2 = a2 - 2*ab + b2\n",
    "    return d2\n",
    "\n",
    "def _kmeanspp_init(X, k, rng):\n",
    "    n_samples = X.shape[0]\n",
    "    centroids = np.empty((k, X.shape[1]), dtype=X.dtype)\n",
    "    # choose first centroid uniformly\n",
    "    idx0 = rng.integers(0, n_samples)\n",
    "    centroids[0] = X[idx0]\n",
    "    # choose remaining with prob proportional to D^2\n",
    "    closest_dist_sq = _euclidean_squared(X, centroids[0:1]).ravel()\n",
    "    for i in range(1, k):\n",
    "        probs = closest_dist_sq / closest_dist_sq.sum()\n",
    "        idx = rng.choice(n_samples, p=probs)\n",
    "        centroids[i] = X[idx]\n",
    "        d2 = _euclidean_squared(X, centroids[i:i+1]).ravel()\n",
    "        closest_dist_sq = np.minimum(closest_dist_sq, d2)\n",
    "    return centroids\n",
    "\n",
    "def kmeans(X, k, max_iter=300, tol=1e-4, init=\"k-means++\", random_state=None):\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    if init == \"k-means++\":\n",
    "        centroids = _kmeanspp_init(X, k, rng)\n",
    "    elif init == \"random\":\n",
    "        indices = rng.choice(X.shape[0], size=k, replace=False)\n",
    "        centroids = X[indices].copy()\n",
    "    else:\n",
    "        raise ValueError(\"init must be 'k-means++' or 'random'\")\n",
    "    \n",
    "    labels = None\n",
    "    for it in range(1, max_iter+1):\n",
    "        # assignment step\n",
    "        d2 = _euclidean_squared(X, centroids)  # (n_samples, k)\n",
    "        new_labels = np.argmin(d2, axis=1)\n",
    "        # update step\n",
    "        new_centroids = centroids.copy()\n",
    "        for j in range(k):\n",
    "            mask = new_labels == j\n",
    "            if np.any(mask):\n",
    "                new_centroids[j] = X[mask].mean(axis=0)\n",
    "            else:\n",
    "                # reinitialize empty cluster to a random point\n",
    "                new_centroids[j] = X[rng.integers(0, X.shape[0])]\n",
    "        # check convergence\n",
    "        shift = np.sqrt(np.sum((new_centroids - centroids)**2))\n",
    "        centroids = new_centroids\n",
    "        if labels is not None and np.all(new_labels == labels):\n",
    "            break\n",
    "        if shift < tol:\n",
    "            labels = new_labels\n",
    "            break\n",
    "        labels = new_labels\n",
    "    \n",
    "    inertia = float(np.sum((X - centroids[labels])**2))\n",
    "    return labels, centroids, inertia, it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecaac87",
   "metadata": {},
   "source": [
    "\n",
    "## Demo on synthetic data\n",
    "We'll create a simple 2D dataset and run our implementation. Then, we'll visualize the clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ac50bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y_true = make_blobs(n_samples=600, centers=4, cluster_std=0.60, random_state=42)\n",
    "labels, centers, inertia, iters = kmeans(X, k=4, random_state=42)\n",
    "\n",
    "print(f\"Inertia: {inertia:.2f}\") \n",
    "print(f\"Iterations: {iters}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0], X[:,1], s=10, alpha=0.6)\n",
    "plt.scatter(centers[:,0], centers[:,1], marker='X', s=200)\n",
    "plt.title(\"K-Means (from scratch) — clusters and centroids\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2293191",
   "metadata": {},
   "source": [
    "\n",
    "## Using scikit-learn\n",
    "The `KMeans` class implements efficient k-means with options for `n_init`, `max_iter`, and `init` strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d790da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=4, init='k-means++', n_init=10, max_iter=300, random_state=42)\n",
    "km.fit(X)\n",
    "print(\"Inertia:\", km.inertia_)\n",
    "print(\"Centroids shape:\", km.cluster_centers_.shape)\n",
    "sk_labels = km.labels_\n",
    "sk_centers = km.cluster_centers_\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0], X[:,1], s=10, alpha=0.6)\n",
    "plt.scatter(sk_centers[:,0], sk_centers[:,1], marker='X', s=200)\n",
    "plt.title(\"K-Means (scikit-learn) — clusters and centroids\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a233642",
   "metadata": {},
   "source": [
    "\n",
    "## Choosing K: Elbow method\n",
    "Run k-means for several values of K and plot inertia vs. K. Look for a point where the decrease in inertia slows down (the \"elbow\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f2a45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Ks = range(1, 10)\n",
    "inertias = []\n",
    "for k in Ks:\n",
    "    km_ = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "    km_.fit(X)\n",
    "    inertias.append(km_.inertia_)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(list(Ks), inertias, marker='o')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Inertia (within-cluster SSE)')\n",
    "plt.title('Elbow method');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36962a1c",
   "metadata": {},
   "source": [
    "\n",
    "## Silhouette score\n",
    "Silhouette measures how similar a point is to its own cluster vs. other clusters (range: -1 to 1). Higher is better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d20810",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "best_k = None\n",
    "best_sil = -1\n",
    "sil_scores = []\n",
    "for k in range(2, 10):\n",
    "    km_ = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "    labels_ = km_.fit_predict(X)\n",
    "    s = silhouette_score(X, labels_)\n",
    "    sil_scores.append(s)\n",
    "    if s > best_sil:\n",
    "        best_sil, best_k = s, k\n",
    "\n",
    "print(f\"Best K by silhouette: {best_k} (score={best_sil:.3f})\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(2,10), sil_scores, marker='o')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.title('Silhouette analysis');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52afe59b",
   "metadata": {},
   "source": [
    "\n",
    "## Preprocessing tips\n",
    "- **Scale features** (e.g., `StandardScaler`) so that all dimensions contribute comparably to Euclidean distance.\n",
    "- **Remove outliers** or use robust scaling if needed.\n",
    "- **Dimensionality reduction** (PCA) can denoise and help visualization.\n",
    "- **Categorical features**: k-means is not ideal for categorical data; consider k-modes/embeddings or a different algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82d3961",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "km_pca = KMeans(n_clusters=4, n_init=10, random_state=42)\n",
    "labels_pca = km_pca.fit_predict(X_pca)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], s=10, alpha=0.6)\n",
    "plt.scatter(km_pca.cluster_centers_[:,0], km_pca.cluster_centers_[:,1], marker='X', s=200)\n",
    "plt.title(\"K-Means on PCA-reduced (2D) data\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3638c18f",
   "metadata": {},
   "source": [
    "\n",
    "## Common pitfalls and gotchas\n",
    "- **Bad initialization** can yield poor local minima → use `k-means++` and multiple `n_init`.\n",
    "- **Scale sensitivity**: unscaled features dominate distance.\n",
    "- **Non-globular clusters**: k-means assumes spherical-ish clusters and equal variance.\n",
    "- **Unequal cluster sizes/densities**: centroid means can be misleading.\n",
    "- **Empty clusters**: can occur; handle by reinitializing or using libraries that do this for you.\n",
    "- **Outliers**: can drag centroids far away; consider trimming or robust alternatives (e.g., k-medoids).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e412755",
   "metadata": {},
   "source": [
    "\n",
    "## Exercises & Question Design (ready to assign)\n",
    "\n",
    "### Conceptual (short answers)\n",
    "1. Explain the difference between **inertia** and **silhouette score**. When can they disagree?\n",
    "2. Why does **feature scaling** matter for k-means? Give an example where not scaling breaks clustering.\n",
    "3. Compare **random initialization** vs. **k-means++**. What problem does k-means++ address?\n",
    "4. Why is k-means not suitable for non-spherical clusters? Provide a counterexample dataset.\n",
    "5. What are common **stopping criteria** for k-means?\n",
    "\n",
    "### Coding (guided)\n",
    "6. Implement the **assignment step** of k-means using NumPy broadcasting for efficiency.\n",
    "7. Extend the from-scratch function to **return the full trace** of inertia per iteration and plot it.\n",
    "8. Add a **mini-batch k-means** variant (hint: update centroids using a random subset each step).\n",
    "9. Implement a **deterministic unit test** using a tiny dataset where the correct centroids are known.\n",
    "\n",
    "### Applied (analysis)\n",
    "10. Use the elbow and silhouette methods to choose K on a real dataset (e.g., Iris without labels).    Report the selected K and justify the choice with both plots and metrics.\n",
    "11. Evaluate the effect of **outliers**: add 5 extreme points to your dataset. How do inertia and centroids change?\n",
    "12. Compare clustering results **with vs. without scaling** on a mixed-scale dataset. Visualize and discuss.\n",
    "\n",
    "### Debugging / Refactoring\n",
    "13. Your k-means run keeps producing **empty clusters**. List three fixes and implement one.\n",
    "14. Your runtime is too slow for 1e6 points. Identify the **bottleneck** and optimize one step (vectorize or mini-batch).\n",
    "15. Your elbow plot has no clear elbow. Propose **three alternative strategies** to pick K and try one.\n",
    "\n",
    "### Grading rubric idea (example)\n",
    "- **Correctness (40%)**: implementation yields stable labels and reasonable inertia.\n",
    "- **Methodology (25%)**: scaling, initialization, and K selection justified.\n",
    "- **Clarity (20%)**: code quality, comments, and plots labeled.\n",
    "- **Insight (15%)**: discussion of limitations and alternatives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf1cfb1",
   "metadata": {},
   "source": [
    "\n",
    "## Quick start (copy-paste)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1157bd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Minimal runnable snippet\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, _ = make_blobs(n_samples=500, centers=3, random_state=0)\n",
    "km = KMeans(n_clusters=3, n_init=10, random_state=0).fit(X)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0], X[:,1], s=10, alpha=0.6)\n",
    "plt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:,1], marker='X', s=200)\n",
    "plt.title(\"K-Means Quick Start\");\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
